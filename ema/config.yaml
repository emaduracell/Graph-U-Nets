# Model hyperparameters
model:
  activation_gnn: "ReLU"
  activation_mlps_final: "ReLU"
  hid_gnn_layer_dim: 128
  hid_mlp_dim: 256
  k_pool_ratios: [0.9, 0.8, 0.7]
  dropout_gnn: 0 # default = 0.1, overfit = 0
  dropout_mlps_final: 0 # default = 0.1, overfit = 0

# Training parameters
training:
  lr: 0.0001 # default = 1e-4 good value 0.001
  epochs: 1000
  batch_size: 4
  shuffle: True # False for overfitting?
  adam_weight_decay: 0 # default: 1e-4 | overfit = 0
  num_train_trajs: 1
  mode: "overfit" # None or "overfit"
  gamma_lr_scheduler: 0.9993 # lr_final = lr0 * gamma^epochs 0.9995 good value
  random_seed: 42 # Seed for train/test split and any random operations
  overfit_traj_id: null # For overfit mode: which trajectory to use (null = use first available)
  overfit_time_idx: null # For overfit mode: which time step t to use (null = use all time steps from trajectory)

