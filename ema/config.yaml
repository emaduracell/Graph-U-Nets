# Model hyperparameters
model:
  activation_gnn: "ReLU"
  activation_mlps_final: "ReLU"
  hid_gnn_layer_dim: 128
  hid_mlp_dim: 256
  k_pool_ratios: [0.9, 0.8, 0.7]
  dropout_gnn: 0 # default = 0.1, overfit = 0
  dropout_mlps_final: 0 # default = 0.1, overfit = 0

# Training parameters
training:
  lr: 0.0001 # default = 1e-4 good value 0.001
  epochs: 1000
  batch_size: 4
  shuffle: False # False for overfitting?
  adam_weight_decay: 0 # default: 1e-4 | overfit = 0
  num_train_trajs: 1
  mode: "overfit" # None or "overfit"
  gamma_lr_scheduler: 0.9993 # lr_final = lr0 * gamma^epochs 0.9995 good value

